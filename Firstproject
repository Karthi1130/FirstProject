package pack
import org.apache.spark.SparkContext
import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf
import org.apache.spark.sql.Row
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.DataFrame
import scala.io.Source
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.catalyst.expressions.Upper
import org.apache.spark.sql.functions.upper
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions.col
object obj1 {
  
  def main(args:Array[String]):Unit={
   
   val conf = new SparkConf().setAppName("Revision").setMaster("local[*]")
   
   val sc =new SparkContext(conf)
   
   sc.setLogLevel("ERROR")
   
   val spark = SparkSession.builder().getOrCreate()
   
   import spark.implicits._
   
 val df = spark.read.format("csv").option("header","true").load("file:///c:/task/emtec.csv")
 
 df.show()
 
  val df2 = spark.read.format("csv").option("header","true").load("file:///c:/task/emtecone.txt")
 
 df2.show()
 
 val df1 = df
 .withColumn("list of fruits", regexp_replace(col("list of fruits"),"[\\[\\]\\']",""))
 .withColumn("list fruits", explode(split(col("list of fruits"),",")))
 .withColumnRenamed("list fruits", "fruit")
 .select("Name","fruit")
df1.show()
 
 val df3 = df1.join(df2 , Seq("fruit") , "left")
 df3.show()
        
